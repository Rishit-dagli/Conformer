{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import tensorflow as tf\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, dim, heads=8, dim_head=64, dropout=0.0, max_pos_emb=512, **kwargs\n",
    "    ):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_q = tf.keras.layers.Dense(inner_dim, use_bias=False)\n",
    "        self.to_kv = tf.keras.layers.Dense(inner_dim * 2, use_bias=False)\n",
    "        self.to_out = tf.keras.layers.Dense(dim)\n",
    "\n",
    "        self.max_pos_emb = max_pos_emb\n",
    "        self.rel_pos_emb = tf.keras.layers.Embedding(2 * max_pos_emb + 1, dim_head)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, context=None, mask=None, context_mask=None):\n",
    "        n = inputs.shape[-2]\n",
    "        heads = self.heads\n",
    "        max_pos_emb = self.max_pos_emb\n",
    "        if context is None:\n",
    "            has_context = False\n",
    "            context = inputs\n",
    "        else:\n",
    "            has_context = True\n",
    "\n",
    "        kv = tf.split(self.to_kv(context), num_or_size_splits=2, axis=-1)\n",
    "        q, k, v = (self.to_q(inputs), *kv)\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=heads), (q, k, v)\n",
    "        )\n",
    "        dots = tf.einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        seq = tf.range(n)\n",
    "        dist = rearrange(seq, \"i -> i ()\") - rearrange(seq, \"j -> () j\")\n",
    "        dist = (\n",
    "            tf.clip_by_value(\n",
    "                dist, clip_value_min=-max_pos_emb, clip_value_max=max_pos_emb\n",
    "            )\n",
    "            + max_pos_emb\n",
    "        )\n",
    "        rel_pos_emb = self.rel_pos_emb(dist)\n",
    "        pos_attn = tf.einsum(\"b h n d, n r d -> b h n r\", q, rel_pos_emb) * self.scale\n",
    "        dots = dots + pos_attn\n",
    "\n",
    "        if mask is not None or context_mask is not None:\n",
    "            if mask is not None:\n",
    "                mask = tf.ones(*inputs.shape[:2])\n",
    "            if not has_context:\n",
    "                if context_mask is None:\n",
    "                    context_mask = mask\n",
    "            else:\n",
    "                if context_mask is None:\n",
    "                    context_mask = tf.ones(*context.shape[:2])\n",
    "            mask_value = -tf.experimental.numpy.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, \"b i -> b () i ()\") * rearrange(\n",
    "                context_mask, \"b j -> b () () j\"\n",
    "            )\n",
    "            dots = tf.where(mask, mask_value, dots)\n",
    "\n",
    "        attn = tf.nn.softmax(dots, axis=-1)\n",
    "\n",
    "        out = tf.einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.to_out(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import tensorflow as tf\n",
    "from einops import rearrange\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "\n",
    "class Swish(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Swish, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * tf.sigmoid(inputs)\n",
    "\n",
    "\n",
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(GLU, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out, gate = tf.split(inputs, 2, axis=self.dim)\n",
    "        return out * tf.sigmoid(gate)\n",
    "\n",
    "\n",
    "class DepthwiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, chan_in, chan_out, kernel_size, padding, **kwargs):\n",
    "        super(DepthwiseLayer, self).__init__(**kwargs)\n",
    "        self.padding = padding\n",
    "        self.chan_in = chan_in\n",
    "        self.conv = tf.keras.layers.Conv1D(chan_out, 1, groups=chan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1])\n",
    "        padded = tf.zeros(\n",
    "            [self.chan_in * self.chan_in] - tf.shape(inputs), dtype=inputs.dtype\n",
    "        )\n",
    "        inputs = tf.concat([inputs, padded], 0)\n",
    "        inputs = tf.reshape(inputs, [-1, self.chan_in, self.chan_in])\n",
    "\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "class Scale(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale, fn, **kwargs):\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "        self.scale = scale\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return self.fn(inputs, **kwargs) * self.scale\n",
    "\n",
    "\n",
    "class PreNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, fn, **kwargs):\n",
    "        super(PreNorm, self).__init__(**kwargs)\n",
    "        self.norm = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        inputs = self.norm(inputs)\n",
    "        return self.fn(inputs, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, mult=4, dropout=0.0, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(dim * mult, activation=Swish()),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(dim, input_dim=dim * mult),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.net(inputs)\n",
    "\n",
    "\n",
    "class BatchNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, causal, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        self.causal = causal\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self.causal:\n",
    "            return tf.keras.layers.BatchNormalization(axis=-1)(inputs)\n",
    "        return tf.identity(inputs)\n",
    "\n",
    "\n",
    "class ConformerConvModule(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        causal=False,\n",
    "        expansion_factor=2,\n",
    "        kernel_size=31,\n",
    "        dropout=0.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ConformerConvModule, self).__init__(**kwargs)\n",
    "\n",
    "        inner_dim = dim * expansion_factor\n",
    "        if not causal:\n",
    "            padding = (kernel_size // 2, kernel_size // 2 - (kernel_size + 1) % 2)\n",
    "        else:\n",
    "            padding = (kernel_size - 1, 0)\n",
    "\n",
    "        self.net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.LayerNormalization(axis=-1),\n",
    "                Rearrange(\"b n c -> b c n\"),\n",
    "                tf.keras.layers.Conv1D(filters=inner_dim * 2, kernel_size=1),\n",
    "                GLU(dim=1),\n",
    "                DepthwiseLayer(\n",
    "                    inner_dim, inner_dim, kernel_size=kernel_size, padding=padding\n",
    "                ),\n",
    "                BatchNorm(causal=causal),\n",
    "                Swish(),\n",
    "                tf.keras.layers.Conv1D(filters=dim, kernel_size=1),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.net(inputs)\n",
    "\n",
    "\n",
    "class ConformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        ff_mult=4,\n",
    "        conv_expansion_factor=2,\n",
    "        conv_kernel_size=31,\n",
    "        attn_dropout=0.0,\n",
    "        ff_dropout=0.0,\n",
    "        conv_dropout=0.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ConformerBlock, self).__init__(**kwargs)\n",
    "        self.ff1 = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout)\n",
    "        self.attn = Attention(\n",
    "            dim=dim, dim_head=dim_head, heads=heads, dropout=attn_dropout\n",
    "        )\n",
    "        self.conv = ConformerConvModule(\n",
    "            dim=dim,\n",
    "            causal=False,\n",
    "            expansion_factor=conv_expansion_factor,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            dropout=conv_dropout,\n",
    "        )\n",
    "        self.ff2 = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout)\n",
    "\n",
    "        self.attn = PreNorm(dim, self.attn)\n",
    "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
    "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
    "\n",
    "        self.post_norm = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        inputs = self.ff1(inputs) + inputs\n",
    "        inputs = self.attn(inputs, mask=mask) + inputs\n",
    "        inputs = self.conv(inputs) + inputs\n",
    "        inputs = self.ff2(inputs) + inputs\n",
    "        inputs = self.post_norm(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1024, 512), dtype=float32, numpy=\n",
       "array([[[-0.00290883, -0.08756144,  1.6656997 , ..., -0.97767735,\n",
       "         -1.1422614 ,  0.19128467],\n",
       "        [ 0.5734962 ,  0.8345062 , -0.27422383, ...,  0.5083876 ,\n",
       "         -1.3008978 ,  1.4405061 ],\n",
       "        [ 0.10144693,  1.3422375 , -0.4186478 , ..., -2.37908   ,\n",
       "         -0.20841315, -0.41334307],\n",
       "        ...,\n",
       "        [-2.0952437 , -1.3566366 , -0.41583762, ..., -1.5943388 ,\n",
       "         -0.39278817, -0.20738427],\n",
       "        [-0.9839722 ,  1.3855118 , -0.9643004 , ...,  0.40495092,\n",
       "          0.57846004,  1.0053539 ],\n",
       "        [-0.04149332,  0.9229126 , -0.53396004, ...,  1.4384449 ,\n",
       "         -0.35634235,  0.04340519]],\n",
       "\n",
       "       [[ 0.65110886, -1.211673  , -0.18238236, ...,  1.2675526 ,\n",
       "          0.6693731 ,  0.8344408 ],\n",
       "        [-1.4031745 , -0.33083993, -0.3024953 , ...,  0.30091405,\n",
       "         -0.65949064, -0.16577375],\n",
       "        [ 1.088807  , -0.36428514,  1.1360062 , ...,  1.8558371 ,\n",
       "         -2.247536  , -1.1256361 ],\n",
       "        ...,\n",
       "        [ 1.423351  ,  1.4339029 ,  0.9061002 , ...,  0.21399657,\n",
       "          0.31782654, -0.5704425 ],\n",
       "        [-1.6198478 ,  0.46314782,  0.6438916 , ..., -1.8042349 ,\n",
       "         -0.86198485,  1.3086736 ],\n",
       "        [-0.29498237, -0.17420727,  1.0921685 , ...,  1.5975716 ,\n",
       "          0.11168142,  2.4761474 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conformer_block = ConformerBlock(\n",
    "    dim=512,\n",
    "    dim_head=64,\n",
    "    heads=8,\n",
    "    ff_mult=4,\n",
    "    conv_expansion_factor=2,\n",
    "    conv_kernel_size=31,\n",
    "    attn_dropout=0.0,\n",
    "    ff_dropout=0.0,\n",
    "    conv_dropout=0.0,\n",
    ")\n",
    "\n",
    "x = tf.random.normal([2, 1024, 512])\n",
    "conformer_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
